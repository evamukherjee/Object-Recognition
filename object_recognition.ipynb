{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "try:\n",
        "    import cv2\n",
        "except ImportError:\n",
        "    install(\"opencv-python\")\n",
        "\n",
        "try:\n",
        "    import mediapipe as mp\n",
        "except ImportError:\n",
        "    install(\"mediapipe\")\n",
        "\n",
        "try:\n",
        "    import time\n",
        "except ImportError:\n",
        "    install(\"time\")\n",
        "\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision"
      ],
      "metadata": {
        "id": "Y9UOf9tdX8xX"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This project  focuses on building a real-time object recognition system designed to detect and classify common objects from a live camera feed. It's developed using Python, MediaPipe, and OpenCV. The main function of the project includes capturing video from a camera, processing each frame to detect objects, and then overlaying the detection results (bounding boxes and labels) onto the live video display.\n",
        "\n"
      ],
      "metadata": {
        "id": "Pd9FJbalnZi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### About the project\n",
        "\n",
        "The primary objective of this project is to develop a computer vision application capable of identifying a small, predefined set of common objects (e.g., chair, phone) within real-time video streams. This system uses pre-trained models of MediaPipe for object detection and classification, complemented by OpenCV for camera access, video processing and display.\n"
      ],
      "metadata": {
        "id": "CHbcYISQNflT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objectives\n",
        "\n",
        "1. **Object Detection with MediaPipe:** The system  implements the MediaPipe library for detecting and recognizing objects (e.g., phone, chair etc.) in real-time from a camera feed. An interface is developed using OpenCV that captures video, processes frames, and displays the detected objects with visual annotations on-screen.\n",
        "\n",
        "3. **Object Classification:** A pre-trained MediaPipe model is employed to classify the detected objects, ensuring that the recognized objects are clearly identified with corresponding labels displayed on the screen."
      ],
      "metadata": {
        "id": "LY1fATpGtMky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing libraries\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import time\n",
        "\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision"
      ],
      "metadata": {
        "id": "axv_uV608pQQ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will define the path to the pre-trained MediaPipe object detection model (`efficientdet_lite0.tflite`). We also initialize a global variable `detection_results`. This variable will store the latest object detection results returned by the MediaPipe asynchronous callback function. The global variable is declared as `None` initially and updated by the `results_callback function`."
      ],
      "metadata": {
        "id": "VygQR4KjENLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'efficientdet_lite0.tflite'"
      ],
      "metadata": {
        "id": "NQoWaT1rDZrf"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detection_results = None"
      ],
      "metadata": {
        "id": "wzD1VDYW_h2P"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MediaPipe Callback Function (`results_callback`)**\n",
        "\n",
        "This function serves as the callback for MediaPipe's asynchronous object detector. Whenever MediaPipe finishes processing a frame and generates detection results, this function is called. It updates the global `detection_results` variable with the most recent detection outcomes, making them available for the main loop to retrieve and display. This is required for the `LIVE_STREAM` running mode.\n",
        "\n"
      ],
      "metadata": {
        "id": "qcGHrTeBoglC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def results_callback(result: vision.ObjectDetectorResult, output_image: mp.Image, timestamp_ms: int):\n",
        "    global detection_results\n",
        "    detection_results = result"
      ],
      "metadata": {
        "id": "787NMd9H_h6n"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialization of Object Detector (`create_live_detector`)**\n",
        "\n",
        "This function is responsible for initializing and configuring the MediaPipe Object Detector. It sets up the detector with specific options tailored for real-time, live stream processing, including the model path, running mode, confidence thresholds, and the callback function for results."
      ],
      "metadata": {
        "id": "QmjEHl9CotXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_live_detector():\n",
        "    base_options = python.BaseOptions(model_asset_path=model_path)\n",
        "    options = vision.ObjectDetectorOptions(\n",
        "        base_options=base_options,\n",
        "        running_mode=vision.RunningMode.LIVE_STREAM,\n",
        "        score_threshold=0.5,\n",
        "        max_results=3,\n",
        "        result_callback=results_callback\n",
        "    )\n",
        "    detector = vision.ObjectDetector.create_from_options(options)\n",
        "    print(\"MediaPipe Object Detector initialized successfully.\")\n",
        "    return detector"
      ],
      "metadata": {
        "id": "lYiXtcLV_h-X"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main function initializes the webcam, establishes a continuous loop to capture video frames, sends these frames to the MediaPipe detector for asynchronous processing, and then displays the detected objects with their labels and the current Frames Per Second (FPS) on a live video feed.\n",
        "\n",
        "1. **Webcam Initialization:** `cv2.VideoCapture(0)` attempts to open the default webcam.\n",
        "\n",
        "2. **Detector Initialization:** Calls `create_live_detector()` to set up the MediaPipe object detector.\n",
        "\n",
        "3. **FPS Calculation Setup:** `prev_time` is initialized to calculate Frames Per Second (FPS) for performance monitoring. <br> <br>\n",
        "\n",
        "The `while cap.isOpened()` loop continuously captures frames from the webcam.\n",
        "\n",
        "* **Frame Capture:** `cap.read()` reads a frame. If reading fails (e.g., end of stream, camera error), the loop continues to the next iteration. <br> <br>\n",
        "\n",
        "4. **Data Preprocessing for MediaPipe:**\n",
        "          \n",
        "* **Color Conversion:** OpenCV captures frames in BGR format, but MediaPipe expects RGB. `cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)` performs this conversion.\n",
        "* **MediaPipe Image Creation:** The `rgb_frame` is converted into an `mp.Image` object, which is the required input format for MediaPipe's detection API. <br> <br>\n",
        "\n",
        "5. **Timestamp Generation:** A unique timestamp in milliseconds (`int(time.time() * 1000)`) is generated for each frame. This is crucial for `LIVE_STREAM` mode to ensure proper processing order and synchronization of results.\n",
        "\n",
        "6. **Asynchronous Detection:** `detector.detect_async(mp_image, timestamp)` sends the image to the MediaPipe detector for processing. This call is non-blocking, allowing the main loop to continue capturing frames while detection happens in the background.\n",
        "\n",
        "7. **Brief Pause:** `time.sleep(0.01)` introduces a small delay. This allows the `results_callback` function (running on a different thread) sufficient time to update detection_results. Without this, there's a higher chance of the main loop trying to access detection_results before it's updated, leading to flickering or missed detections. <br> <BR>\n",
        "\n",
        "8. **Process and Display Detection Results:** If `detection_results` is not `None` (meaning detections have been received from the callback), the code iterates through each detected object:\n",
        "\n",
        "* It extracts the `bounding_box` coordinates, `category_name`, and `score`\n",
        "\n",
        "* A green rectangle (`cv2.rectangle`) is drawn around the detected object using its bounding box coordinates\n",
        "\n",
        "* A label string is created, combining the class name and the confidence score (formatted as a percentage)\n",
        "   \n",
        "* The label text is placed above the bounding box (`cv2.putText`) <br> <br>  \n",
        "\n",
        "9. **FPS Display:** Calculates and displays the current FPS on the top-left corner of the frame.\n",
        "\n",
        "10. **Frame Display:** `cv2.imshow(\"Async Object Detection\" frame)` displays the processed frame with bounding boxes, labels and FPS.\n",
        "\n",
        "11. **Quit Condition:** Pressing the 'q' key breaks the loop, releasing the webcam resources and destroying all OpenCV windows."
      ],
      "metadata": {
        "id": "N1fL5glG6jx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    global detection_results\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    detector = create_live_detector()\n",
        "\n",
        "    prev_time = 0\n",
        "    print(\" Press 'q' to quit.\")\n",
        "\n",
        "    while cap.isOpened():\n",
        "        success, frame = cap.read()\n",
        "        if not success:\n",
        "            continue\n",
        "\n",
        "        # Data Preprocessing for MediaPipe\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
        "\n",
        "        timestamp = int(time.time() * 1000)\n",
        "        detector.detect_async(mp_image, timestamp)\n",
        "\n",
        "        time.sleep(0.01)\n",
        "\n",
        "        # Process and Display Detection Results\n",
        "        if detection_results:\n",
        "            for detection in detection_results.detections:\n",
        "                bbox = detection.bounding_box\n",
        "                category = detection.categories[0]\n",
        "                class_name = category.category_name\n",
        "                score = category.score\n",
        "\n",
        "                x, y, w, h = bbox.origin_x, bbox.origin_y, bbox.width, bbox.height\n",
        "\n",
        "                 # Define colors\n",
        "                person_color = (0, 0, 255)  # Red for person (BGR format)\n",
        "                other_object_color = (0, 255, 0) # Green for other objects\n",
        "\n",
        "                # Determine color based on class name\n",
        "                if class_name.lower() == 'person':\n",
        "                    box_color = person_color\n",
        "                else:\n",
        "                    box_color = other_object_color\n",
        "\n",
        "\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), box_color, 2)\n",
        "                label = f\"{class_name} ({int(score * 100)}%)\"\n",
        "                cv2.putText(frame, label, (x, y - 10),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, box_color, 2)\n",
        "\n",
        "        # FPS display\n",
        "        curr_time = time.time()\n",
        "        fps = 1 / (curr_time - prev_time) if prev_time else 0\n",
        "        prev_time = curr_time\n",
        "        cv2.putText(frame, f'FPS: {int(fps)}', (10, 30),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
        "\n",
        "        # Show the Processed frame in a window named \"Async Object Detection\"\n",
        "        cv2.imshow(\"Async Object Detection\", frame)\n",
        "\n",
        "        # Quit Condition\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMWSpv9ruBG1",
        "outputId": "a0f8e410-5a55-46a6-bd01-e3382d20177c"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MediaPipe Object Detector initialized successfully.\n",
            " Press 'q' to quit.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Technical Choices and Algorithms\n",
        "\n",
        "1. **Technologies used:**\n",
        "    \n",
        "   * **MediaPipe:** It's selected for its pre-trained machine learning models specifically designed for various vision tasks, including object detection. MediaPipe provides a streamlined API for integrating ML models into real-time applications.\n",
        "\n",
        "   * **OpenCV (Open Source Computer Vision Library):** It is used for camera interaction (capturing video frames), image preprocessing (e.g., color conversion, flipping), and visualizing the detection results (drawing bounding boxes, labels, and displaying the video stream).\n",
        "\n",
        "2. **Object Detection Model:** The system uses MediaPipe's pre-trained `efficientdet_lite0.tflite` model to detect a small set of common objects (e.g., phone, person etc.).\n",
        "\n",
        "3. **Asynchronous Processing:** MediaPipe's `LIVE_STREAM` running mode is utilized for asynchronous detection, which helps in maintaining a smooth frame rate by processing frames in the background while the main thread continues to display the video feed.\n",
        "    \n",
        "4. **Performance Monitoring:** Frames Per Second (FPS) is displayed on the video feed to give an indication of the system's performance.\n",
        "\n",
        "5. **Data Preprocessing:** Frames captured in BGR format by OpenCV are converted to RGB for MediaPipe processing. A unique timestamp is generated for each frame to ensure proper processing order in `LIVE_STREAM` mode.\n",
        "\n",
        "6. **Visualization:** OpenCV's drawing utilities (`cv2.rectangle`, `cv2.putText`) are used to overlay the detection results onto the original video frames. This includes drawing bounding boxes around detected objects and displaying their predicted class labels along with confidence scores."
      ],
      "metadata": {
        "id": "7eoAP5YHDQtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observation\n",
        "\n",
        "The `efficientdet_lite0.tflite` model was primarily trained on the COCO dataset, which includes 80 common object categories. While objects like person, cell phone and chair are well-represented and have distinct features, smaller objects like pen and pencil might be difficult for a lightweight model trained on a general dataset, especially if their scale in the input image is very small or if they lack distinctive features compared to background elements.\n",
        "\n",
        "* A higher threshold (e.g., 0.5) prioritizes precision. The system detects large, distinct, and common objects such as person, cell phone, chair and cup. These objects typically yield high confidence scores from the efficientdet_lite0 model. A higher threshold filters out low-confidence predictions, leading to fewer false positives but potentially missing objects that the model is less confident about.\n",
        "\n",
        "* A lower threshold (e.g., 0.3) prioritizes recall. It allows more detections to pass through, including those with marginal confidence. While this can help detect some previously missed objects, it includes incorrect predictions, resulting in the observed false positives. The efficientdet_lite0 model, being lightweight, is more prone to these ambiguities at lower confidence levels, especially for objects that are visually similar such as pencil or pen to other classes or have less distinctive features in the training data.\n",
        "\n"
      ],
      "metadata": {
        "id": "Zeye8pUcXpvd"
      }
    }
  ]
}